{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Danish_text_generator_LSTM_pyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNSE89BUA3GDqr6kQVmV1SV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sharaborina/ChatBot/blob/main/Danish_text_generator_LSTM_pyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJIqXiRsPtM2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87a059a1-0c44-43f5-95b7-9f9c4964ea4d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoRf_rmkCTRe"
      },
      "source": [
        "\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "import torch\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUAul0BSGaNz"
      },
      "source": [
        "import json\n",
        "def load_json(directory, file):\n",
        "  with open(f'{directory}/{file}') as f:\n",
        "    db = json.load(f)\n",
        "  return db\n",
        "\n",
        "def upload_json(directory, file, db):\n",
        "  with open(f'{directory}/{file}', mode='w') as f:\n",
        "    json.dump(db,f)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVBi0hwgkEyU"
      },
      "source": [
        "Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_p-SPtRHqlM",
        "outputId": "fc69dea8-b408-4c90-b5d7-63a5d082ad95"
      },
      "source": [
        "da = \"/content/drive/MyDrive/ColabNotebooks/MultiWoz/untokenized_data_danish.txt\"\n",
        "text = load_json('/content/drive/MyDrive/ColabNotebooks/MultiWoz', 'untokenized_data_danish.txt')\n",
        "text = ' '.join(text)\n",
        "print(text[:500])"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Person 1: er på udkig efter et sted at bo, der har en billig prisinterval, det skal være i en type hotel Person 2: Okay, har du et bestemt område, du vil bo i? Person 1: nej, jeg skal bare sørge for, at det er billigt. åh, og jeg har brug for parkering Person 2: Jeg fandt et billigt hotel til dig, der inkluderer parkering. Kan du lide, at jeg bestiller det? Person 1: Ja tak. 6 personer 3 nætter startende på tirsdag. Person 2: Jeg er ked af det, men jeg kunne ikke booke det til dig til tirsdag. E\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc57yOlZHdkI"
      },
      "source": [
        "# Create two dictionaries:\n",
        "# int2char -- maps integers to characters\n",
        "# char2int -- maps characters to unique integers\n",
        "import string\n",
        "# chars = string.ascii_letters + \".,;:\"\n",
        "chars = sorted(set(list(text)))\n",
        "int2char, char2int = {}, {}\n",
        "for i,c in enumerate(chars):\n",
        "  int2char[i] = c\n",
        "  char2int[c] = i\n",
        "\n",
        "\n",
        "\n",
        "upload_json('/content/drive/MyDrive/ColabNotebooks/MultiWoz', 'chars.txt', chars)\n",
        "\n",
        "\n",
        "\n",
        "# encode the text\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7AHjZDjHdmS"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "def one_hot_encode(arr, n_labels):\n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
        "    \n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    \n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    \n",
        "    return one_hot"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aOF1Ma1Hdon"
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "    # Create a generator that returns batches of size batch_size x seq_length\n",
        "    batch_size_total = batch_size * seq_length\n",
        "    ## Get the number of batches we can make\n",
        "    n_batches = len(arr)//batch_size_total\n",
        "    \n",
        "    ## Keep only enough characters to make full batches\n",
        "    arr = arr[:n_batches * batch_size_total]\n",
        "    ## Reshape into batch_size rows\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "    ## Iterate over the batches using a window of size seq_length\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        x = arr[:, n:n +seq_length]\n",
        "        # The target is a version of x shifted by one (do not forget border conditions)\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJdTAnthHdsY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f12f2b29-9886-4cca-86c4-40235c37890a"
      },
      "source": [
        "# check if GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU')\n",
        "else: \n",
        "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnEgO4QmIFob"
      },
      "source": [
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
        "                               drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        self.vocab_size = len(tokens)\n",
        "        self.input_dim = n_hidden\n",
        "        self.output_size = len(tokens)\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.vocab_size)\n",
        "\n",
        "        # creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        # Define the LSTM layer\n",
        "        self.lstm = lstm_layer = nn.LSTM(self.input_dim, n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        # Define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.rnn = getattr(nn, 'LSTM')(len(self.chars), n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
        "\n",
        "        # Define the final, fully-connected output layer        \n",
        "        self.fc = nn.Linear(self.n_hidden, self.output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self.decoder = nn.Linear(n_hidden, len(self.chars))\n",
        "\n",
        "        self.init_weights()\n",
        "        self.cuda()\n",
        "        \n",
        "    def forward(self, x, hidden):\n",
        "        ''' Forward pass through the network '''\n",
        "        x, h = self.rnn(x, hidden)\n",
        "        x = self.dropout(x)\n",
        "        # x = x.view(x.size(0)*x.size(1), self.n_hidden)\n",
        "        x = x.reshape(x.size(0)*x.size(1), self.n_hidden)\n",
        "        x = self.decoder(x)\n",
        "\n",
        "        return x, h\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        \n",
        "        return hidden\n",
        "\n",
        "    def init_weights(self):\n",
        "        ''' Initialize weights of decoder (fully connected layer) '''\n",
        "\n",
        "        # Apply bias tensor to all zeros\n",
        "        self.decoder.bias.data.fill_(0)\n",
        "\n",
        "        # Apply random uniform weights to decoder\n",
        "        self.decoder.weight.data.uniform_(-1, 1)\n",
        "\n",
        "        "
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW-XFHb6L9-0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPw2jl-rIJxv"
      },
      "source": [
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    ''' Training a network \n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "    \n",
        "    '''\n",
        "    net.train()\n",
        "    \n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            \n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            if(train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            net.zero_grad()\n",
        "            \n",
        "            output, h = net(inputs, h)\n",
        "            \n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward()\n",
        "\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "            \n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                net.train() # reset to train mode after iterationg through validation data\n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PujVcm1IP2V"
      },
      "source": [
        ""
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSzfdB5PIQFw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ce15b02-d556-4410-f020-29919dd9667b"
      },
      "source": [
        "# Set model hyperparameters\n",
        "\n",
        "n_hidden = 256\n",
        "n_layers = 5\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharRNN(\n",
            "  (embedding): Embedding(97, 97)\n",
            "  (lstm): LSTM(256, 256, num_layers=5, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (rnn): LSTM(97, 256, num_layers=5, batch_first=True, dropout=0.5)\n",
            "  (fc): Linear(in_features=256, out_features=97, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            "  (decoder): Linear(in_features=256, out_features=97, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtIjLy7PIVLS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d9bbc2f-41c0-4539-b90f-9328908d76ac"
      },
      "source": [
        "# set testing hyperparameters\n",
        "\n",
        "batch_size = 20\n",
        "seq_length = 500\n",
        "# start small if you are just testing initial behavior\n",
        "n_epochs = 5\n",
        "\n",
        "# train the model\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "net.to(device)\n",
        "\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.01, print_every=10)\n"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/5... Step: 10... Loss: 3.4969... Val Loss: 3.3854\n",
            "Epoch: 1/5... Step: 20... Loss: 3.3192... Val Loss: 3.2077\n",
            "Epoch: 1/5... Step: 30... Loss: 3.2481... Val Loss: 3.1741\n",
            "Epoch: 1/5... Step: 40... Loss: 3.1247... Val Loss: 2.9669\n",
            "Epoch: 1/5... Step: 50... Loss: 2.7272... Val Loss: 2.6054\n",
            "Epoch: 1/5... Step: 60... Loss: 2.5381... Val Loss: 2.4160\n",
            "Epoch: 1/5... Step: 70... Loss: 2.4492... Val Loss: 2.3208\n",
            "Epoch: 1/5... Step: 80... Loss: 2.3814... Val Loss: 2.2437\n",
            "Epoch: 1/5... Step: 90... Loss: 2.2821... Val Loss: 2.1718\n",
            "Epoch: 1/5... Step: 100... Loss: 2.1917... Val Loss: 2.0961\n",
            "Epoch: 1/5... Step: 110... Loss: 2.1817... Val Loss: 2.0153\n",
            "Epoch: 1/5... Step: 120... Loss: 2.0771... Val Loss: 1.9304\n",
            "Epoch: 1/5... Step: 130... Loss: 1.9736... Val Loss: 1.8703\n",
            "Epoch: 1/5... Step: 140... Loss: 1.9135... Val Loss: 1.7996\n",
            "Epoch: 1/5... Step: 150... Loss: 1.8345... Val Loss: 1.7184\n",
            "Epoch: 1/5... Step: 160... Loss: 1.8312... Val Loss: 1.6300\n",
            "Epoch: 1/5... Step: 170... Loss: 1.7701... Val Loss: 1.5688\n",
            "Epoch: 1/5... Step: 180... Loss: 1.6162... Val Loss: 1.4992\n",
            "Epoch: 1/5... Step: 190... Loss: 1.6213... Val Loss: 1.4365\n",
            "Epoch: 1/5... Step: 200... Loss: 1.5753... Val Loss: 1.3795\n",
            "Epoch: 1/5... Step: 210... Loss: 1.5332... Val Loss: 1.3354\n",
            "Epoch: 1/5... Step: 220... Loss: 1.4995... Val Loss: 1.2944\n",
            "Epoch: 1/5... Step: 230... Loss: 1.4978... Val Loss: 1.2522\n",
            "Epoch: 1/5... Step: 240... Loss: 1.3913... Val Loss: 1.2164\n",
            "Epoch: 1/5... Step: 250... Loss: 1.3581... Val Loss: 1.1881\n",
            "Epoch: 1/5... Step: 260... Loss: 1.2893... Val Loss: 1.1623\n",
            "Epoch: 1/5... Step: 270... Loss: 1.3261... Val Loss: 1.1353\n",
            "Epoch: 1/5... Step: 280... Loss: 1.2521... Val Loss: 1.0940\n",
            "Epoch: 1/5... Step: 290... Loss: 1.1855... Val Loss: 1.0723\n",
            "Epoch: 1/5... Step: 300... Loss: 1.1600... Val Loss: 1.0565\n",
            "Epoch: 1/5... Step: 310... Loss: 1.2672... Val Loss: 1.0321\n",
            "Epoch: 1/5... Step: 320... Loss: 1.1752... Val Loss: 1.0098\n",
            "Epoch: 1/5... Step: 330... Loss: 1.1147... Val Loss: 0.9916\n",
            "Epoch: 1/5... Step: 340... Loss: 1.1161... Val Loss: 0.9787\n",
            "Epoch: 1/5... Step: 350... Loss: 1.0965... Val Loss: 0.9668\n",
            "Epoch: 1/5... Step: 360... Loss: 1.1088... Val Loss: 0.9559\n",
            "Epoch: 1/5... Step: 370... Loss: 1.0873... Val Loss: 0.9408\n",
            "Epoch: 1/5... Step: 380... Loss: 1.0816... Val Loss: 0.9266\n",
            "Epoch: 1/5... Step: 390... Loss: 1.1000... Val Loss: 0.9165\n",
            "Epoch: 1/5... Step: 400... Loss: 1.0547... Val Loss: 0.9077\n",
            "Epoch: 1/5... Step: 410... Loss: 1.0706... Val Loss: 0.8976\n",
            "Epoch: 1/5... Step: 420... Loss: 1.0616... Val Loss: 0.8904\n",
            "Epoch: 1/5... Step: 430... Loss: 1.0814... Val Loss: 0.8815\n",
            "Epoch: 1/5... Step: 440... Loss: 1.0771... Val Loss: 0.8706\n",
            "Epoch: 1/5... Step: 450... Loss: 1.0655... Val Loss: 0.8646\n",
            "Epoch: 1/5... Step: 460... Loss: 0.9739... Val Loss: 0.8573\n",
            "Epoch: 1/5... Step: 470... Loss: 0.9743... Val Loss: 0.8503\n",
            "Epoch: 1/5... Step: 480... Loss: 1.0197... Val Loss: 0.8416\n",
            "Epoch: 1/5... Step: 490... Loss: 1.0348... Val Loss: 0.8385\n",
            "Epoch: 1/5... Step: 500... Loss: 0.9697... Val Loss: 0.8322\n",
            "Epoch: 1/5... Step: 510... Loss: 0.9470... Val Loss: 0.8230\n",
            "Epoch: 1/5... Step: 520... Loss: 1.0108... Val Loss: 0.8193\n",
            "Epoch: 1/5... Step: 530... Loss: 0.9113... Val Loss: 0.8161\n",
            "Epoch: 1/5... Step: 540... Loss: 0.9313... Val Loss: 0.8114\n",
            "Epoch: 1/5... Step: 550... Loss: 0.9469... Val Loss: 0.8069\n",
            "Epoch: 1/5... Step: 560... Loss: 0.9298... Val Loss: 0.8024\n",
            "Epoch: 1/5... Step: 570... Loss: 0.9208... Val Loss: 0.7954\n",
            "Epoch: 1/5... Step: 580... Loss: 0.8838... Val Loss: 0.7961\n",
            "Epoch: 1/5... Step: 590... Loss: 0.9784... Val Loss: 0.7924\n",
            "Epoch: 1/5... Step: 600... Loss: 0.9283... Val Loss: 0.7870\n",
            "Epoch: 1/5... Step: 610... Loss: 0.8687... Val Loss: 0.7829\n",
            "Epoch: 1/5... Step: 620... Loss: 0.9496... Val Loss: 0.7777\n",
            "Epoch: 1/5... Step: 630... Loss: 0.9109... Val Loss: 0.7738\n",
            "Epoch: 1/5... Step: 640... Loss: 0.9246... Val Loss: 0.7687\n",
            "Epoch: 1/5... Step: 650... Loss: 0.8769... Val Loss: 0.7690\n",
            "Epoch: 1/5... Step: 660... Loss: 0.9458... Val Loss: 0.7650\n",
            "Epoch: 1/5... Step: 670... Loss: 0.8807... Val Loss: 0.7588\n",
            "Epoch: 1/5... Step: 680... Loss: 0.9168... Val Loss: 0.7604\n",
            "Epoch: 1/5... Step: 690... Loss: 0.8387... Val Loss: 0.7566\n",
            "Epoch: 1/5... Step: 700... Loss: 0.8929... Val Loss: 0.7558\n",
            "Epoch: 1/5... Step: 710... Loss: 0.8360... Val Loss: 0.7500\n",
            "Epoch: 1/5... Step: 720... Loss: 0.8659... Val Loss: 0.7447\n",
            "Epoch: 1/5... Step: 730... Loss: 0.8564... Val Loss: 0.7469\n",
            "Epoch: 1/5... Step: 740... Loss: 0.8369... Val Loss: 0.7446\n",
            "Epoch: 1/5... Step: 750... Loss: 0.8960... Val Loss: 0.7425\n",
            "Epoch: 1/5... Step: 760... Loss: 0.8749... Val Loss: 0.7376\n",
            "Epoch: 1/5... Step: 770... Loss: 0.8836... Val Loss: 0.7382\n",
            "Epoch: 1/5... Step: 780... Loss: 0.9012... Val Loss: 0.7350\n",
            "Epoch: 1/5... Step: 790... Loss: 0.8827... Val Loss: 0.7337\n",
            "Epoch: 1/5... Step: 800... Loss: 0.8366... Val Loss: 0.7333\n",
            "Epoch: 1/5... Step: 810... Loss: 0.8822... Val Loss: 0.7291\n",
            "Epoch: 1/5... Step: 820... Loss: 0.8334... Val Loss: 0.7257\n",
            "Epoch: 1/5... Step: 830... Loss: 0.8572... Val Loss: 0.7208\n",
            "Epoch: 1/5... Step: 840... Loss: 0.9065... Val Loss: 0.7223\n",
            "Epoch: 1/5... Step: 850... Loss: 0.7963... Val Loss: 0.7211\n",
            "Epoch: 1/5... Step: 860... Loss: 0.8864... Val Loss: 0.7206\n",
            "Epoch: 1/5... Step: 870... Loss: 0.8707... Val Loss: 0.7202\n",
            "Epoch: 1/5... Step: 880... Loss: 0.8377... Val Loss: 0.7152\n",
            "Epoch: 1/5... Step: 890... Loss: 0.8187... Val Loss: 0.7127\n",
            "Epoch: 1/5... Step: 900... Loss: 0.8409... Val Loss: 0.7128\n",
            "Epoch: 1/5... Step: 910... Loss: 0.8453... Val Loss: 0.7113\n",
            "Epoch: 1/5... Step: 920... Loss: 0.8357... Val Loss: 0.7103\n",
            "Epoch: 1/5... Step: 930... Loss: 0.8398... Val Loss: 0.7083\n",
            "Epoch: 1/5... Step: 940... Loss: 0.7677... Val Loss: 0.7056\n",
            "Epoch: 1/5... Step: 950... Loss: 0.8722... Val Loss: 0.7047\n",
            "Epoch: 1/5... Step: 960... Loss: 0.8230... Val Loss: 0.7064\n",
            "Epoch: 1/5... Step: 970... Loss: 0.7957... Val Loss: 0.6993\n",
            "Epoch: 1/5... Step: 980... Loss: 0.7996... Val Loss: 0.7000\n",
            "Epoch: 1/5... Step: 990... Loss: 0.8415... Val Loss: 0.7007\n",
            "Epoch: 1/5... Step: 1000... Loss: 0.7973... Val Loss: 0.6998\n",
            "Epoch: 1/5... Step: 1010... Loss: 0.8233... Val Loss: 0.6958\n",
            "Epoch: 1/5... Step: 1020... Loss: 0.7908... Val Loss: 0.6957\n",
            "Epoch: 2/5... Step: 1030... Loss: 0.8080... Val Loss: 0.6943\n",
            "Epoch: 2/5... Step: 1040... Loss: 0.8605... Val Loss: 0.6912\n",
            "Epoch: 2/5... Step: 1050... Loss: 0.8230... Val Loss: 0.6902\n",
            "Epoch: 2/5... Step: 1060... Loss: 0.8013... Val Loss: 0.6907\n",
            "Epoch: 2/5... Step: 1070... Loss: 0.8196... Val Loss: 0.6895\n",
            "Epoch: 2/5... Step: 1080... Loss: 0.7470... Val Loss: 0.6864\n",
            "Epoch: 2/5... Step: 1090... Loss: 0.7925... Val Loss: 0.6870\n",
            "Epoch: 2/5... Step: 1100... Loss: 0.7987... Val Loss: 0.6848\n",
            "Epoch: 2/5... Step: 1110... Loss: 0.8452... Val Loss: 0.6844\n",
            "Epoch: 2/5... Step: 1120... Loss: 0.7597... Val Loss: 0.6829\n",
            "Epoch: 2/5... Step: 1130... Loss: 0.7479... Val Loss: 0.6800\n",
            "Epoch: 2/5... Step: 1140... Loss: 0.8098... Val Loss: 0.6785\n",
            "Epoch: 2/5... Step: 1150... Loss: 0.7879... Val Loss: 0.6767\n",
            "Epoch: 2/5... Step: 1160... Loss: 0.8053... Val Loss: 0.6758\n",
            "Epoch: 2/5... Step: 1170... Loss: 0.7945... Val Loss: 0.6753\n",
            "Epoch: 2/5... Step: 1180... Loss: 0.7795... Val Loss: 0.6731\n",
            "Epoch: 2/5... Step: 1190... Loss: 0.8063... Val Loss: 0.6747\n",
            "Epoch: 2/5... Step: 1200... Loss: 0.7835... Val Loss: 0.6708\n",
            "Epoch: 2/5... Step: 1210... Loss: 0.8071... Val Loss: 0.6726\n",
            "Epoch: 2/5... Step: 1220... Loss: 0.7524... Val Loss: 0.6723\n",
            "Epoch: 2/5... Step: 1230... Loss: 0.7815... Val Loss: 0.6700\n",
            "Epoch: 2/5... Step: 1240... Loss: 0.8153... Val Loss: 0.6693\n",
            "Epoch: 2/5... Step: 1250... Loss: 0.7930... Val Loss: 0.6680\n",
            "Epoch: 2/5... Step: 1260... Loss: 0.8192... Val Loss: 0.6677\n",
            "Epoch: 2/5... Step: 1270... Loss: 0.7494... Val Loss: 0.6666\n",
            "Epoch: 2/5... Step: 1280... Loss: 0.7904... Val Loss: 0.6657\n",
            "Epoch: 2/5... Step: 1290... Loss: 0.8148... Val Loss: 0.6645\n",
            "Epoch: 2/5... Step: 1300... Loss: 0.7811... Val Loss: 0.6612\n",
            "Epoch: 2/5... Step: 1310... Loss: 0.7400... Val Loss: 0.6624\n",
            "Epoch: 2/5... Step: 1320... Loss: 0.7736... Val Loss: 0.6606\n",
            "Epoch: 2/5... Step: 1330... Loss: 0.8081... Val Loss: 0.6566\n",
            "Epoch: 2/5... Step: 1340... Loss: 0.8180... Val Loss: 0.6570\n",
            "Epoch: 2/5... Step: 1350... Loss: 0.7854... Val Loss: 0.6581\n",
            "Epoch: 2/5... Step: 1360... Loss: 0.7752... Val Loss: 0.6568\n",
            "Epoch: 2/5... Step: 1370... Loss: 0.7940... Val Loss: 0.6544\n",
            "Epoch: 2/5... Step: 1380... Loss: 0.7814... Val Loss: 0.6554\n",
            "Epoch: 2/5... Step: 1390... Loss: 0.7387... Val Loss: 0.6563\n",
            "Epoch: 2/5... Step: 1400... Loss: 0.7323... Val Loss: 0.6552\n",
            "Epoch: 2/5... Step: 1410... Loss: 0.7564... Val Loss: 0.6533\n",
            "Epoch: 2/5... Step: 1420... Loss: 0.7479... Val Loss: 0.6561\n",
            "Epoch: 2/5... Step: 1430... Loss: 0.7584... Val Loss: 0.6541\n",
            "Epoch: 2/5... Step: 1440... Loss: 0.7616... Val Loss: 0.6540\n",
            "Epoch: 2/5... Step: 1450... Loss: 0.7373... Val Loss: 0.6516\n",
            "Epoch: 2/5... Step: 1460... Loss: 0.7535... Val Loss: 0.6485\n",
            "Epoch: 2/5... Step: 1470... Loss: 0.7022... Val Loss: 0.6473\n",
            "Epoch: 2/5... Step: 1480... Loss: 0.7559... Val Loss: 0.6458\n",
            "Epoch: 2/5... Step: 1490... Loss: 0.7407... Val Loss: 0.6474\n",
            "Epoch: 2/5... Step: 1500... Loss: 0.7661... Val Loss: 0.6472\n",
            "Epoch: 2/5... Step: 1510... Loss: 0.7655... Val Loss: 0.6463\n",
            "Epoch: 2/5... Step: 1520... Loss: 0.7870... Val Loss: 0.6457\n",
            "Epoch: 2/5... Step: 1530... Loss: 0.7611... Val Loss: 0.6453\n",
            "Epoch: 2/5... Step: 1540... Loss: 0.7280... Val Loss: 0.6424\n",
            "Epoch: 2/5... Step: 1550... Loss: 0.7221... Val Loss: 0.6424\n",
            "Epoch: 2/5... Step: 1560... Loss: 0.7608... Val Loss: 0.6410\n",
            "Epoch: 2/5... Step: 1570... Loss: 0.7370... Val Loss: 0.6418\n",
            "Epoch: 2/5... Step: 1580... Loss: 0.7174... Val Loss: 0.6419\n",
            "Epoch: 2/5... Step: 1590... Loss: 0.7295... Val Loss: 0.6401\n",
            "Epoch: 2/5... Step: 1600... Loss: 0.7433... Val Loss: 0.6397\n",
            "Epoch: 2/5... Step: 1610... Loss: 0.7201... Val Loss: 0.6403\n",
            "Epoch: 2/5... Step: 1620... Loss: 0.7841... Val Loss: 0.6381\n",
            "Epoch: 2/5... Step: 1630... Loss: 0.7627... Val Loss: 0.6396\n",
            "Epoch: 2/5... Step: 1640... Loss: 0.7088... Val Loss: 0.6374\n",
            "Epoch: 2/5... Step: 1650... Loss: 0.7860... Val Loss: 0.6351\n",
            "Epoch: 2/5... Step: 1660... Loss: 0.7459... Val Loss: 0.6336\n",
            "Epoch: 2/5... Step: 1670... Loss: 0.7495... Val Loss: 0.6328\n",
            "Epoch: 2/5... Step: 1680... Loss: 0.7777... Val Loss: 0.6343\n",
            "Epoch: 2/5... Step: 1690... Loss: 0.7958... Val Loss: 0.6346\n",
            "Epoch: 2/5... Step: 1700... Loss: 0.8208... Val Loss: 0.6336\n",
            "Epoch: 2/5... Step: 1710... Loss: 0.6799... Val Loss: 0.6318\n",
            "Epoch: 2/5... Step: 1720... Loss: 0.7680... Val Loss: 0.6327\n",
            "Epoch: 2/5... Step: 1730... Loss: 0.7546... Val Loss: 0.6294\n",
            "Epoch: 2/5... Step: 1740... Loss: 0.7371... Val Loss: 0.6293\n",
            "Epoch: 2/5... Step: 1750... Loss: 0.7434... Val Loss: 0.6285\n",
            "Epoch: 2/5... Step: 1760... Loss: 0.7430... Val Loss: 0.6289\n",
            "Epoch: 2/5... Step: 1770... Loss: 0.7565... Val Loss: 0.6279\n",
            "Epoch: 2/5... Step: 1780... Loss: 0.7277... Val Loss: 0.6268\n",
            "Epoch: 2/5... Step: 1790... Loss: 0.7305... Val Loss: 0.6271\n",
            "Epoch: 2/5... Step: 1800... Loss: 0.7582... Val Loss: 0.6251\n",
            "Epoch: 2/5... Step: 1810... Loss: 0.7347... Val Loss: 0.6250\n",
            "Epoch: 2/5... Step: 1820... Loss: 0.7034... Val Loss: 0.6279\n",
            "Epoch: 2/5... Step: 1830... Loss: 0.7728... Val Loss: 0.6290\n",
            "Epoch: 2/5... Step: 1840... Loss: 0.7941... Val Loss: 0.6246\n",
            "Epoch: 2/5... Step: 1850... Loss: 0.7964... Val Loss: 0.6222\n",
            "Epoch: 2/5... Step: 1860... Loss: 0.6831... Val Loss: 0.6247\n",
            "Epoch: 2/5... Step: 1870... Loss: 0.7420... Val Loss: 0.6219\n",
            "Epoch: 2/5... Step: 1880... Loss: 0.7425... Val Loss: 0.6227\n",
            "Epoch: 2/5... Step: 1890... Loss: 0.7354... Val Loss: 0.6237\n",
            "Epoch: 2/5... Step: 1900... Loss: 0.7110... Val Loss: 0.6237\n",
            "Epoch: 2/5... Step: 1910... Loss: 0.7276... Val Loss: 0.6232\n",
            "Epoch: 2/5... Step: 1920... Loss: 0.7005... Val Loss: 0.6226\n",
            "Epoch: 2/5... Step: 1930... Loss: 0.7386... Val Loss: 0.6210\n",
            "Epoch: 2/5... Step: 1940... Loss: 0.7026... Val Loss: 0.6212\n",
            "Epoch: 2/5... Step: 1950... Loss: 0.7152... Val Loss: 0.6209\n",
            "Epoch: 2/5... Step: 1960... Loss: 0.7289... Val Loss: 0.6183\n",
            "Epoch: 2/5... Step: 1970... Loss: 0.7448... Val Loss: 0.6185\n",
            "Epoch: 2/5... Step: 1980... Loss: 0.7359... Val Loss: 0.6184\n",
            "Epoch: 2/5... Step: 1990... Loss: 0.7057... Val Loss: 0.6179\n",
            "Epoch: 2/5... Step: 2000... Loss: 0.6555... Val Loss: 0.6169\n",
            "Epoch: 2/5... Step: 2010... Loss: 0.7493... Val Loss: 0.6162\n",
            "Epoch: 2/5... Step: 2020... Loss: 0.7505... Val Loss: 0.6183\n",
            "Epoch: 2/5... Step: 2030... Loss: 0.6966... Val Loss: 0.6165\n",
            "Epoch: 2/5... Step: 2040... Loss: 0.7877... Val Loss: 0.6161\n",
            "Epoch: 2/5... Step: 2050... Loss: 0.7358... Val Loss: 0.6144\n",
            "Epoch: 3/5... Step: 2060... Loss: 0.6684... Val Loss: 0.6161\n",
            "Epoch: 3/5... Step: 2070... Loss: 0.6684... Val Loss: 0.6145\n",
            "Epoch: 3/5... Step: 2080... Loss: 0.7856... Val Loss: 0.6140\n",
            "Epoch: 3/5... Step: 2090... Loss: 0.7534... Val Loss: 0.6140\n",
            "Epoch: 3/5... Step: 2100... Loss: 0.7388... Val Loss: 0.6128\n",
            "Epoch: 3/5... Step: 2110... Loss: 0.7089... Val Loss: 0.6128\n",
            "Epoch: 3/5... Step: 2120... Loss: 0.7538... Val Loss: 0.6146\n",
            "Epoch: 3/5... Step: 2130... Loss: 0.7481... Val Loss: 0.6132\n",
            "Epoch: 3/5... Step: 2140... Loss: 0.7566... Val Loss: 0.6118\n",
            "Epoch: 3/5... Step: 2150... Loss: 0.7089... Val Loss: 0.6093\n",
            "Epoch: 3/5... Step: 2160... Loss: 0.6677... Val Loss: 0.6100\n",
            "Epoch: 3/5... Step: 2170... Loss: 0.6965... Val Loss: 0.6095\n",
            "Epoch: 3/5... Step: 2180... Loss: 0.7184... Val Loss: 0.6107\n",
            "Epoch: 3/5... Step: 2190... Loss: 0.7315... Val Loss: 0.6077\n",
            "Epoch: 3/5... Step: 2200... Loss: 0.7031... Val Loss: 0.6091\n",
            "Epoch: 3/5... Step: 2210... Loss: 0.7002... Val Loss: 0.6083\n",
            "Epoch: 3/5... Step: 2220... Loss: 0.6684... Val Loss: 0.6087\n",
            "Epoch: 3/5... Step: 2230... Loss: 0.7237... Val Loss: 0.6065\n",
            "Epoch: 3/5... Step: 2240... Loss: 0.6799... Val Loss: 0.6098\n",
            "Epoch: 3/5... Step: 2250... Loss: 0.6870... Val Loss: 0.6079\n",
            "Epoch: 3/5... Step: 2260... Loss: 0.6968... Val Loss: 0.6068\n",
            "Epoch: 3/5... Step: 2270... Loss: 0.7117... Val Loss: 0.6055\n",
            "Epoch: 3/5... Step: 2280... Loss: 0.7203... Val Loss: 0.6053\n",
            "Epoch: 3/5... Step: 2290... Loss: 0.7176... Val Loss: 0.6039\n",
            "Epoch: 3/5... Step: 2300... Loss: 0.7930... Val Loss: 0.6022\n",
            "Epoch: 3/5... Step: 2310... Loss: 0.6646... Val Loss: 0.6049\n",
            "Epoch: 3/5... Step: 2320... Loss: 0.7187... Val Loss: 0.6043\n",
            "Epoch: 3/5... Step: 2330... Loss: 0.6716... Val Loss: 0.6035\n",
            "Epoch: 3/5... Step: 2340... Loss: 0.7180... Val Loss: 0.6029\n",
            "Epoch: 3/5... Step: 2350... Loss: 0.7174... Val Loss: 0.6040\n",
            "Epoch: 3/5... Step: 2360... Loss: 0.7303... Val Loss: 0.6025\n",
            "Epoch: 3/5... Step: 2370... Loss: 0.6646... Val Loss: 0.6020\n",
            "Epoch: 3/5... Step: 2380... Loss: 0.6966... Val Loss: 0.6017\n",
            "Epoch: 3/5... Step: 2390... Loss: 0.7366... Val Loss: 0.6024\n",
            "Epoch: 3/5... Step: 2400... Loss: 0.6811... Val Loss: 0.6010\n",
            "Epoch: 3/5... Step: 2410... Loss: 0.6648... Val Loss: 0.6026\n",
            "Epoch: 3/5... Step: 2420... Loss: 0.7182... Val Loss: 0.6023\n",
            "Epoch: 3/5... Step: 2430... Loss: 0.7098... Val Loss: 0.6028\n",
            "Epoch: 3/5... Step: 2440... Loss: 0.6920... Val Loss: 0.6000\n",
            "Epoch: 3/5... Step: 2450... Loss: 0.6394... Val Loss: 0.6012\n",
            "Epoch: 3/5... Step: 2460... Loss: 0.6671... Val Loss: 0.6020\n",
            "Epoch: 3/5... Step: 2470... Loss: 0.7340... Val Loss: 0.5998\n",
            "Epoch: 3/5... Step: 2480... Loss: 0.6589... Val Loss: 0.6001\n",
            "Epoch: 3/5... Step: 2490... Loss: 0.7073... Val Loss: 0.5979\n",
            "Epoch: 3/5... Step: 2500... Loss: 0.6930... Val Loss: 0.5978\n",
            "Epoch: 3/5... Step: 2510... Loss: 0.7056... Val Loss: 0.5993\n",
            "Epoch: 3/5... Step: 2520... Loss: 0.6771... Val Loss: 0.5971\n",
            "Epoch: 3/5... Step: 2530... Loss: 0.6832... Val Loss: 0.5981\n",
            "Epoch: 3/5... Step: 2540... Loss: 0.7452... Val Loss: 0.5971\n",
            "Epoch: 3/5... Step: 2550... Loss: 0.6630... Val Loss: 0.5970\n",
            "Epoch: 3/5... Step: 2560... Loss: 0.6932... Val Loss: 0.5953\n",
            "Epoch: 3/5... Step: 2570... Loss: 0.7298... Val Loss: 0.5956\n",
            "Epoch: 3/5... Step: 2580... Loss: 0.6644... Val Loss: 0.5965\n",
            "Epoch: 3/5... Step: 2590... Loss: 0.6849... Val Loss: 0.5954\n",
            "Epoch: 3/5... Step: 2600... Loss: 0.6544... Val Loss: 0.5958\n",
            "Epoch: 3/5... Step: 2610... Loss: 0.6655... Val Loss: 0.5962\n",
            "Epoch: 3/5... Step: 2620... Loss: 0.6979... Val Loss: 0.5944\n",
            "Epoch: 3/5... Step: 2630... Loss: 0.6786... Val Loss: 0.5936\n",
            "Epoch: 3/5... Step: 2640... Loss: 0.7350... Val Loss: 0.5941\n",
            "Epoch: 3/5... Step: 2650... Loss: 0.7040... Val Loss: 0.5924\n",
            "Epoch: 3/5... Step: 2660... Loss: 0.6513... Val Loss: 0.5949\n",
            "Epoch: 3/5... Step: 2670... Loss: 0.6677... Val Loss: 0.5943\n",
            "Epoch: 3/5... Step: 2680... Loss: 0.6446... Val Loss: 0.5909\n",
            "Epoch: 3/5... Step: 2690... Loss: 0.6993... Val Loss: 0.5915\n",
            "Epoch: 3/5... Step: 2700... Loss: 0.6748... Val Loss: 0.5912\n",
            "Epoch: 3/5... Step: 2710... Loss: 0.6478... Val Loss: 0.5892\n",
            "Epoch: 3/5... Step: 2720... Loss: 0.6739... Val Loss: 0.5905\n",
            "Epoch: 3/5... Step: 2730... Loss: 0.6733... Val Loss: 0.5906\n",
            "Epoch: 3/5... Step: 2740... Loss: 0.6574... Val Loss: 0.5893\n",
            "Epoch: 3/5... Step: 2750... Loss: 0.6516... Val Loss: 0.5914\n",
            "Epoch: 3/5... Step: 2760... Loss: 0.6809... Val Loss: 0.5889\n",
            "Epoch: 3/5... Step: 2770... Loss: 0.6364... Val Loss: 0.5883\n",
            "Epoch: 3/5... Step: 2780... Loss: 0.6437... Val Loss: 0.5883\n",
            "Epoch: 3/5... Step: 2790... Loss: 0.6766... Val Loss: 0.5874\n",
            "Epoch: 3/5... Step: 2800... Loss: 0.6975... Val Loss: 0.5860\n",
            "Epoch: 3/5... Step: 2810... Loss: 0.6501... Val Loss: 0.5866\n",
            "Epoch: 3/5... Step: 2820... Loss: 0.6222... Val Loss: 0.5849\n",
            "Epoch: 3/5... Step: 2830... Loss: 0.7019... Val Loss: 0.5845\n",
            "Epoch: 3/5... Step: 2840... Loss: 0.7239... Val Loss: 0.5833\n",
            "Epoch: 3/5... Step: 2850... Loss: 0.6741... Val Loss: 0.5847\n",
            "Epoch: 3/5... Step: 2860... Loss: 0.6810... Val Loss: 0.5860\n",
            "Epoch: 3/5... Step: 2870... Loss: 0.6705... Val Loss: 0.5838\n",
            "Epoch: 3/5... Step: 2880... Loss: 0.6919... Val Loss: 0.5829\n",
            "Epoch: 3/5... Step: 2890... Loss: 0.6680... Val Loss: 0.5829\n",
            "Epoch: 3/5... Step: 2900... Loss: 0.6388... Val Loss: 0.5817\n",
            "Epoch: 3/5... Step: 2910... Loss: 0.7019... Val Loss: 0.5818\n",
            "Epoch: 3/5... Step: 2920... Loss: 0.6609... Val Loss: 0.5813\n",
            "Epoch: 3/5... Step: 2930... Loss: 0.6492... Val Loss: 0.5820\n",
            "Epoch: 3/5... Step: 2940... Loss: 0.6611... Val Loss: 0.5807\n",
            "Epoch: 3/5... Step: 2950... Loss: 0.6756... Val Loss: 0.5808\n",
            "Epoch: 3/5... Step: 2960... Loss: 0.6383... Val Loss: 0.5813\n",
            "Epoch: 3/5... Step: 2970... Loss: 0.6730... Val Loss: 0.5820\n",
            "Epoch: 3/5... Step: 2980... Loss: 0.7040... Val Loss: 0.5820\n",
            "Epoch: 3/5... Step: 2990... Loss: 0.6723... Val Loss: 0.5799\n",
            "Epoch: 3/5... Step: 3000... Loss: 0.7101... Val Loss: 0.5797\n",
            "Epoch: 3/5... Step: 3010... Loss: 0.6708... Val Loss: 0.5794\n",
            "Epoch: 3/5... Step: 3020... Loss: 0.6842... Val Loss: 0.5784\n",
            "Epoch: 3/5... Step: 3030... Loss: 0.7160... Val Loss: 0.5806\n",
            "Epoch: 3/5... Step: 3040... Loss: 0.6193... Val Loss: 0.5782\n",
            "Epoch: 3/5... Step: 3050... Loss: 0.7159... Val Loss: 0.5793\n",
            "Epoch: 3/5... Step: 3060... Loss: 0.6666... Val Loss: 0.5777\n",
            "Epoch: 3/5... Step: 3070... Loss: 0.6773... Val Loss: 0.5785\n",
            "Epoch: 3/5... Step: 3080... Loss: 0.7166... Val Loss: 0.5781\n",
            "Epoch: 4/5... Step: 3090... Loss: 0.7062... Val Loss: 0.5782\n",
            "Epoch: 4/5... Step: 3100... Loss: 0.6671... Val Loss: 0.5770\n",
            "Epoch: 4/5... Step: 3110... Loss: 0.6691... Val Loss: 0.5765\n",
            "Epoch: 4/5... Step: 3120... Loss: 0.6830... Val Loss: 0.5764\n",
            "Epoch: 4/5... Step: 3130... Loss: 0.6689... Val Loss: 0.5776\n",
            "Epoch: 4/5... Step: 3140... Loss: 0.7003... Val Loss: 0.5758\n",
            "Epoch: 4/5... Step: 3150... Loss: 0.6920... Val Loss: 0.5751\n",
            "Epoch: 4/5... Step: 3160... Loss: 0.6672... Val Loss: 0.5768\n",
            "Epoch: 4/5... Step: 3170... Loss: 0.6857... Val Loss: 0.5754\n",
            "Epoch: 4/5... Step: 3180... Loss: 0.6139... Val Loss: 0.5760\n",
            "Epoch: 4/5... Step: 3190... Loss: 0.7279... Val Loss: 0.5745\n",
            "Epoch: 4/5... Step: 3200... Loss: 0.6588... Val Loss: 0.5756\n",
            "Epoch: 4/5... Step: 3210... Loss: 0.6804... Val Loss: 0.5787\n",
            "Epoch: 4/5... Step: 3220... Loss: 0.6893... Val Loss: 0.5788\n",
            "Epoch: 4/5... Step: 3230... Loss: 0.6852... Val Loss: 0.5793\n",
            "Epoch: 4/5... Step: 3240... Loss: 0.7028... Val Loss: 0.5780\n",
            "Epoch: 4/5... Step: 3250... Loss: 0.7065... Val Loss: 0.5789\n",
            "Epoch: 4/5... Step: 3260... Loss: 0.6858... Val Loss: 0.5769\n",
            "Epoch: 4/5... Step: 3270... Loss: 0.6559... Val Loss: 0.5775\n",
            "Epoch: 4/5... Step: 3280... Loss: 0.6489... Val Loss: 0.5763\n",
            "Epoch: 4/5... Step: 3290... Loss: 0.6816... Val Loss: 0.5748\n",
            "Epoch: 4/5... Step: 3300... Loss: 0.6328... Val Loss: 0.5741\n",
            "Epoch: 4/5... Step: 3310... Loss: 0.6906... Val Loss: 0.5743\n",
            "Epoch: 4/5... Step: 3320... Loss: 0.7009... Val Loss: 0.5739\n",
            "Epoch: 4/5... Step: 3330... Loss: 0.6684... Val Loss: 0.5724\n",
            "Epoch: 4/5... Step: 3340... Loss: 0.6412... Val Loss: 0.5726\n",
            "Epoch: 4/5... Step: 3350... Loss: 0.6683... Val Loss: 0.5715\n",
            "Epoch: 4/5... Step: 3360... Loss: 0.6853... Val Loss: 0.5725\n",
            "Epoch: 4/5... Step: 3370... Loss: 0.6207... Val Loss: 0.5714\n",
            "Epoch: 4/5... Step: 3380... Loss: 0.5959... Val Loss: 0.5714\n",
            "Epoch: 4/5... Step: 3390... Loss: 0.6807... Val Loss: 0.5710\n",
            "Epoch: 4/5... Step: 3400... Loss: 0.6826... Val Loss: 0.5699\n",
            "Epoch: 4/5... Step: 3410... Loss: 0.6924... Val Loss: 0.5696\n",
            "Epoch: 4/5... Step: 3420... Loss: 0.6665... Val Loss: 0.5702\n",
            "Epoch: 4/5... Step: 3430... Loss: 0.6786... Val Loss: 0.5696\n",
            "Epoch: 4/5... Step: 3440... Loss: 0.6983... Val Loss: 0.5710\n",
            "Epoch: 4/5... Step: 3450... Loss: 0.6922... Val Loss: 0.5704\n",
            "Epoch: 4/5... Step: 3460... Loss: 0.6612... Val Loss: 0.5710\n",
            "Epoch: 4/5... Step: 3470... Loss: 0.6504... Val Loss: 0.5693\n",
            "Epoch: 4/5... Step: 3480... Loss: 0.7126... Val Loss: 0.5695\n",
            "Epoch: 4/5... Step: 3490... Loss: 0.7300... Val Loss: 0.5699\n",
            "Epoch: 4/5... Step: 3500... Loss: 0.6307... Val Loss: 0.5695\n",
            "Epoch: 4/5... Step: 3510... Loss: 0.6136... Val Loss: 0.5705\n",
            "Epoch: 4/5... Step: 3520... Loss: 0.7043... Val Loss: 0.5684\n",
            "Epoch: 4/5... Step: 3530... Loss: 0.6848... Val Loss: 0.5689\n",
            "Epoch: 4/5... Step: 3540... Loss: 0.6477... Val Loss: 0.5697\n",
            "Epoch: 4/5... Step: 3550... Loss: 0.7004... Val Loss: 0.5683\n",
            "Epoch: 4/5... Step: 3560... Loss: 0.6212... Val Loss: 0.5661\n",
            "Epoch: 4/5... Step: 3570... Loss: 0.6695... Val Loss: 0.5659\n",
            "Epoch: 4/5... Step: 3580... Loss: 0.6400... Val Loss: 0.5667\n",
            "Epoch: 4/5... Step: 3590... Loss: 0.6824... Val Loss: 0.5673\n",
            "Epoch: 4/5... Step: 3600... Loss: 0.7414... Val Loss: 0.5670\n",
            "Epoch: 4/5... Step: 3610... Loss: 0.6680... Val Loss: 0.5668\n",
            "Epoch: 4/5... Step: 3620... Loss: 0.6418... Val Loss: 0.5656\n",
            "Epoch: 4/5... Step: 3630... Loss: 0.6269... Val Loss: 0.5668\n",
            "Epoch: 4/5... Step: 3640... Loss: 0.6454... Val Loss: 0.5680\n",
            "Epoch: 4/5... Step: 3650... Loss: 0.6207... Val Loss: 0.5672\n",
            "Epoch: 4/5... Step: 3660... Loss: 0.6462... Val Loss: 0.5659\n",
            "Epoch: 4/5... Step: 3670... Loss: 0.6638... Val Loss: 0.5643\n",
            "Epoch: 4/5... Step: 3680... Loss: 0.6662... Val Loss: 0.5647\n",
            "Epoch: 4/5... Step: 3690... Loss: 0.6351... Val Loss: 0.5656\n",
            "Epoch: 4/5... Step: 3700... Loss: 0.6515... Val Loss: 0.5653\n",
            "Epoch: 4/5... Step: 3710... Loss: 0.6111... Val Loss: 0.5650\n",
            "Epoch: 4/5... Step: 3720... Loss: 0.6418... Val Loss: 0.5632\n",
            "Epoch: 4/5... Step: 3730... Loss: 0.6757... Val Loss: 0.5637\n",
            "Epoch: 4/5... Step: 3740... Loss: 0.6175... Val Loss: 0.5634\n",
            "Epoch: 4/5... Step: 3750... Loss: 0.6699... Val Loss: 0.5631\n",
            "Epoch: 4/5... Step: 3760... Loss: 0.6630... Val Loss: 0.5647\n",
            "Epoch: 4/5... Step: 3770... Loss: 0.6690... Val Loss: 0.5638\n",
            "Epoch: 4/5... Step: 3780... Loss: 0.7092... Val Loss: 0.5634\n",
            "Epoch: 4/5... Step: 3790... Loss: 0.6511... Val Loss: 0.5622\n",
            "Epoch: 4/5... Step: 3800... Loss: 0.7027... Val Loss: 0.5617\n",
            "Epoch: 4/5... Step: 3810... Loss: 0.6317... Val Loss: 0.5625\n",
            "Epoch: 4/5... Step: 3820... Loss: 0.6140... Val Loss: 0.5621\n",
            "Epoch: 4/5... Step: 3830... Loss: 0.6331... Val Loss: 0.5617\n",
            "Epoch: 4/5... Step: 3840... Loss: 0.6285... Val Loss: 0.5632\n",
            "Epoch: 4/5... Step: 3850... Loss: 0.6770... Val Loss: 0.5628\n",
            "Epoch: 4/5... Step: 3860... Loss: 0.6726... Val Loss: 0.5613\n",
            "Epoch: 4/5... Step: 3870... Loss: 0.6119... Val Loss: 0.5601\n",
            "Epoch: 4/5... Step: 3880... Loss: 0.6854... Val Loss: 0.5604\n",
            "Epoch: 4/5... Step: 3890... Loss: 0.7097... Val Loss: 0.5622\n",
            "Epoch: 4/5... Step: 3900... Loss: 0.6425... Val Loss: 0.5611\n",
            "Epoch: 4/5... Step: 3910... Loss: 0.6375... Val Loss: 0.5603\n",
            "Epoch: 4/5... Step: 3920... Loss: 0.6424... Val Loss: 0.5611\n",
            "Epoch: 4/5... Step: 3930... Loss: 0.6170... Val Loss: 0.5599\n",
            "Epoch: 4/5... Step: 3940... Loss: 0.6355... Val Loss: 0.5594\n",
            "Epoch: 4/5... Step: 3950... Loss: 0.6640... Val Loss: 0.5601\n",
            "Epoch: 4/5... Step: 3960... Loss: 0.6144... Val Loss: 0.5608\n",
            "Epoch: 4/5... Step: 3970... Loss: 0.6641... Val Loss: 0.5609\n",
            "Epoch: 4/5... Step: 3980... Loss: 0.6173... Val Loss: 0.5611\n",
            "Epoch: 4/5... Step: 3990... Loss: 0.6926... Val Loss: 0.5596\n",
            "Epoch: 4/5... Step: 4000... Loss: 0.6204... Val Loss: 0.5617\n",
            "Epoch: 4/5... Step: 4010... Loss: 0.6183... Val Loss: 0.5607\n",
            "Epoch: 4/5... Step: 4020... Loss: 0.6697... Val Loss: 0.5587\n",
            "Epoch: 4/5... Step: 4030... Loss: 0.6565... Val Loss: 0.5603\n",
            "Epoch: 4/5... Step: 4040... Loss: 0.6535... Val Loss: 0.5583\n",
            "Epoch: 4/5... Step: 4050... Loss: 0.6741... Val Loss: 0.5587\n",
            "Epoch: 4/5... Step: 4060... Loss: 0.6416... Val Loss: 0.5593\n",
            "Epoch: 4/5... Step: 4070... Loss: 0.6622... Val Loss: 0.5591\n",
            "Epoch: 4/5... Step: 4080... Loss: 0.6793... Val Loss: 0.5579\n",
            "Epoch: 4/5... Step: 4090... Loss: 0.6333... Val Loss: 0.5575\n",
            "Epoch: 4/5... Step: 4100... Loss: 0.6649... Val Loss: 0.5584\n",
            "Epoch: 4/5... Step: 4110... Loss: 0.6982... Val Loss: 0.5575\n",
            "Epoch: 5/5... Step: 4120... Loss: 0.6248... Val Loss: 0.5583\n",
            "Epoch: 5/5... Step: 4130... Loss: 0.6477... Val Loss: 0.5591\n",
            "Epoch: 5/5... Step: 4140... Loss: 0.6285... Val Loss: 0.5579\n",
            "Epoch: 5/5... Step: 4150... Loss: 0.6836... Val Loss: 0.5558\n",
            "Epoch: 5/5... Step: 4160... Loss: 0.6293... Val Loss: 0.5569\n",
            "Epoch: 5/5... Step: 4170... Loss: 0.6622... Val Loss: 0.5588\n",
            "Epoch: 5/5... Step: 4180... Loss: 0.6849... Val Loss: 0.5587\n",
            "Epoch: 5/5... Step: 4190... Loss: 0.6861... Val Loss: 0.5580\n",
            "Epoch: 5/5... Step: 4200... Loss: 0.6505... Val Loss: 0.5566\n",
            "Epoch: 5/5... Step: 4210... Loss: 0.6575... Val Loss: 0.5570\n",
            "Epoch: 5/5... Step: 4220... Loss: 0.6541... Val Loss: 0.5570\n",
            "Epoch: 5/5... Step: 4230... Loss: 0.6900... Val Loss: 0.5568\n",
            "Epoch: 5/5... Step: 4240... Loss: 0.6643... Val Loss: 0.5580\n",
            "Epoch: 5/5... Step: 4250... Loss: 0.6257... Val Loss: 0.5557\n",
            "Epoch: 5/5... Step: 4260... Loss: 0.5943... Val Loss: 0.5552\n",
            "Epoch: 5/5... Step: 4270... Loss: 0.6896... Val Loss: 0.5571\n",
            "Epoch: 5/5... Step: 4280... Loss: 0.6405... Val Loss: 0.5565\n",
            "Epoch: 5/5... Step: 4290... Loss: 0.6024... Val Loss: 0.5571\n",
            "Epoch: 5/5... Step: 4300... Loss: 0.6445... Val Loss: 0.5552\n",
            "Epoch: 5/5... Step: 4310... Loss: 0.6094... Val Loss: 0.5544\n",
            "Epoch: 5/5... Step: 4320... Loss: 0.7017... Val Loss: 0.5535\n",
            "Epoch: 5/5... Step: 4330... Loss: 0.6296... Val Loss: 0.5550\n",
            "Epoch: 5/5... Step: 4340... Loss: 0.6844... Val Loss: 0.5549\n",
            "Epoch: 5/5... Step: 4350... Loss: 0.6937... Val Loss: 0.5550\n",
            "Epoch: 5/5... Step: 4360... Loss: 0.6621... Val Loss: 0.5533\n",
            "Epoch: 5/5... Step: 4370... Loss: 0.5932... Val Loss: 0.5550\n",
            "Epoch: 5/5... Step: 4380... Loss: 0.6548... Val Loss: 0.5546\n",
            "Epoch: 5/5... Step: 4390... Loss: 0.6548... Val Loss: 0.5532\n",
            "Epoch: 5/5... Step: 4400... Loss: 0.6301... Val Loss: 0.5536\n",
            "Epoch: 5/5... Step: 4410... Loss: 0.5934... Val Loss: 0.5565\n",
            "Epoch: 5/5... Step: 4420... Loss: 0.6757... Val Loss: 0.5531\n",
            "Epoch: 5/5... Step: 4430... Loss: 0.6285... Val Loss: 0.5528\n",
            "Epoch: 5/5... Step: 4440... Loss: 0.6701... Val Loss: 0.5532\n",
            "Epoch: 5/5... Step: 4450... Loss: 0.6734... Val Loss: 0.5526\n",
            "Epoch: 5/5... Step: 4460... Loss: 0.6788... Val Loss: 0.5527\n",
            "Epoch: 5/5... Step: 4470... Loss: 0.6472... Val Loss: 0.5529\n",
            "Epoch: 5/5... Step: 4480... Loss: 0.6035... Val Loss: 0.5526\n",
            "Epoch: 5/5... Step: 4490... Loss: 0.7149... Val Loss: 0.5524\n",
            "Epoch: 5/5... Step: 4500... Loss: 0.6725... Val Loss: 0.5524\n",
            "Epoch: 5/5... Step: 4510... Loss: 0.7015... Val Loss: 0.5544\n",
            "Epoch: 5/5... Step: 4520... Loss: 0.6369... Val Loss: 0.5548\n",
            "Epoch: 5/5... Step: 4530... Loss: 0.6597... Val Loss: 0.5549\n",
            "Epoch: 5/5... Step: 4540... Loss: 0.6219... Val Loss: 0.5521\n",
            "Epoch: 5/5... Step: 4550... Loss: 0.6132... Val Loss: 0.5524\n",
            "Epoch: 5/5... Step: 4560... Loss: 0.6901... Val Loss: 0.5514\n",
            "Epoch: 5/5... Step: 4570... Loss: 0.5969... Val Loss: 0.5527\n",
            "Epoch: 5/5... Step: 4580... Loss: 0.6339... Val Loss: 0.5526\n",
            "Epoch: 5/5... Step: 4590... Loss: 0.6513... Val Loss: 0.5512\n",
            "Epoch: 5/5... Step: 4600... Loss: 0.6263... Val Loss: 0.5506\n",
            "Epoch: 5/5... Step: 4610... Loss: 0.6042... Val Loss: 0.5521\n",
            "Epoch: 5/5... Step: 4620... Loss: 0.6946... Val Loss: 0.5528\n",
            "Epoch: 5/5... Step: 4630... Loss: 0.6463... Val Loss: 0.5514\n",
            "Epoch: 5/5... Step: 4640... Loss: 0.6536... Val Loss: 0.5519\n",
            "Epoch: 5/5... Step: 4650... Loss: 0.6479... Val Loss: 0.5511\n",
            "Epoch: 5/5... Step: 4660... Loss: 0.6562... Val Loss: 0.5514\n",
            "Epoch: 5/5... Step: 4670... Loss: 0.6949... Val Loss: 0.5509\n",
            "Epoch: 5/5... Step: 4680... Loss: 0.6578... Val Loss: 0.5500\n",
            "Epoch: 5/5... Step: 4690... Loss: 0.6603... Val Loss: 0.5502\n",
            "Epoch: 5/5... Step: 4700... Loss: 0.6080... Val Loss: 0.5507\n",
            "Epoch: 5/5... Step: 4710... Loss: 0.6270... Val Loss: 0.5504\n",
            "Epoch: 5/5... Step: 4720... Loss: 0.6163... Val Loss: 0.5508\n",
            "Epoch: 5/5... Step: 4730... Loss: 0.7335... Val Loss: 0.5497\n",
            "Epoch: 5/5... Step: 4740... Loss: 0.7053... Val Loss: 0.5513\n",
            "Epoch: 5/5... Step: 4750... Loss: 0.6457... Val Loss: 0.5502\n",
            "Epoch: 5/5... Step: 4760... Loss: 0.6486... Val Loss: 0.5489\n",
            "Epoch: 5/5... Step: 4770... Loss: 0.6498... Val Loss: 0.5500\n",
            "Epoch: 5/5... Step: 4780... Loss: 0.6329... Val Loss: 0.5498\n",
            "Epoch: 5/5... Step: 4790... Loss: 0.6815... Val Loss: 0.5504\n",
            "Epoch: 5/5... Step: 4800... Loss: 0.6608... Val Loss: 0.5488\n",
            "Epoch: 5/5... Step: 4810... Loss: 0.6387... Val Loss: 0.5480\n",
            "Epoch: 5/5... Step: 4820... Loss: 0.6408... Val Loss: 0.5486\n",
            "Epoch: 5/5... Step: 4830... Loss: 0.6326... Val Loss: 0.5470\n",
            "Epoch: 5/5... Step: 4840... Loss: 0.6369... Val Loss: 0.5487\n",
            "Epoch: 5/5... Step: 4850... Loss: 0.6542... Val Loss: 0.5490\n",
            "Epoch: 5/5... Step: 4860... Loss: 0.6399... Val Loss: 0.5485\n",
            "Epoch: 5/5... Step: 4870... Loss: 0.6250... Val Loss: 0.5474\n",
            "Epoch: 5/5... Step: 4880... Loss: 0.6573... Val Loss: 0.5479\n",
            "Epoch: 5/5... Step: 4890... Loss: 0.6376... Val Loss: 0.5479\n",
            "Epoch: 5/5... Step: 4900... Loss: 0.6020... Val Loss: 0.5470\n",
            "Epoch: 5/5... Step: 4910... Loss: 0.6733... Val Loss: 0.5469\n",
            "Epoch: 5/5... Step: 4920... Loss: 0.6263... Val Loss: 0.5467\n",
            "Epoch: 5/5... Step: 4930... Loss: 0.5949... Val Loss: 0.5473\n",
            "Epoch: 5/5... Step: 4940... Loss: 0.5990... Val Loss: 0.5491\n",
            "Epoch: 5/5... Step: 4950... Loss: 0.6290... Val Loss: 0.5474\n",
            "Epoch: 5/5... Step: 4960... Loss: 0.6246... Val Loss: 0.5471\n",
            "Epoch: 5/5... Step: 4970... Loss: 0.6170... Val Loss: 0.5478\n",
            "Epoch: 5/5... Step: 4980... Loss: 0.5885... Val Loss: 0.5477\n",
            "Epoch: 5/5... Step: 4990... Loss: 0.6722... Val Loss: 0.5464\n",
            "Epoch: 5/5... Step: 5000... Loss: 0.6649... Val Loss: 0.5472\n",
            "Epoch: 5/5... Step: 5010... Loss: 0.6538... Val Loss: 0.5473\n",
            "Epoch: 5/5... Step: 5020... Loss: 0.6286... Val Loss: 0.5468\n",
            "Epoch: 5/5... Step: 5030... Loss: 0.6029... Val Loss: 0.5481\n",
            "Epoch: 5/5... Step: 5040... Loss: 0.6471... Val Loss: 0.5463\n",
            "Epoch: 5/5... Step: 5050... Loss: 0.6043... Val Loss: 0.5454\n",
            "Epoch: 5/5... Step: 5060... Loss: 0.6185... Val Loss: 0.5463\n",
            "Epoch: 5/5... Step: 5070... Loss: 0.6336... Val Loss: 0.5447\n",
            "Epoch: 5/5... Step: 5080... Loss: 0.6504... Val Loss: 0.5446\n",
            "Epoch: 5/5... Step: 5090... Loss: 0.6491... Val Loss: 0.5451\n",
            "Epoch: 5/5... Step: 5100... Loss: 0.6419... Val Loss: 0.5446\n",
            "Epoch: 5/5... Step: 5110... Loss: 0.6170... Val Loss: 0.5460\n",
            "Epoch: 5/5... Step: 5120... Loss: 0.6010... Val Loss: 0.5466\n",
            "Epoch: 5/5... Step: 5130... Loss: 0.6792... Val Loss: 0.5453\n",
            "Epoch: 5/5... Step: 5140... Loss: 0.6594... Val Loss: 0.5450\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fZAZz36IXZu"
      },
      "source": [
        "torch.save(net.state_dict(), '/content/drive/MyDrive/ColabNotebooks/Generative_model_Sharaborina_state_dict.pt' )"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jf_3xNcBIklY"
      },
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "        \n",
        "        # tensor inputs\n",
        "        x = np.array([[net.char2int[char]]])\n",
        "        x = one_hot_encode(x, len(net.chars))\n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        # detach hidden state from history\n",
        "        h = tuple([each.data for each in h])\n",
        "        # get the output of the model\n",
        "        out, h = net(inputs, h)\n",
        "\n",
        "        # get the character probabilities\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "        \n",
        "        # get top characters\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(net.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        # select the likely next character with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return net.int2char[char], h"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lImQdor-InaE"
      },
      "source": [
        "def sample(net, size, prime='The', top_k=None):\n",
        "        \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval() # eval mode\n",
        "    \n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HeNKSFFIrXp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4063e1e-cc97-414d-940c-a053403b9940"
      },
      "source": [
        "print(sample(net, 300, prime='Er der hund?', top_k=2))"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Er der hund? Person 1: Ja det skal være i centrum af byen. Person 2: Der er flere, hvis du har brug for at rejse efter 10:15 Person 1: Jeg leder også efter oplysninger om et hotel i centrum af byen og en moderat prisklasse. Person 2: Ja, det er et hotel i den østlige del af byen og er dyrt. Vil du have mig til a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3NMRJp-wSY6"
      },
      "source": [
        ""
      ],
      "execution_count": 92,
      "outputs": []
    }
  ]
}