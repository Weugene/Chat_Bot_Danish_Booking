{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spacy NER.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyON875N3Ha25F9TABRbuOMu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sharaborina/ChatBot/blob/main/Spacy_NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tBI4XUUw9DF"
      },
      "source": [
        "# Development of NER model based on pretrained Spacy model for Danish langauage and MultiWoz dataset\n",
        "\n",
        "Here a model for extraction different entities (date, city, name of hotel, parking etc. ) is presented. For further training of Spacy model `da_core_news_lg `, the MultiWoz dataset was chosen thanks to the fact that all dialogues were marked up.\n",
        "\n",
        "##1.1 Connection to the Google drive, installation of libraries and importing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94ZSrT7iacIR",
        "outputId": "0e266284-0f32-486f-8e00-e4f9a1a5bac3"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MPkLq8SXYcz"
      },
      "source": [
        "!pip uninstall -y  spacy --quiet\n",
        "!pip install spacy --quiet\n",
        "!python -m spacy download da_core_news_lg >out 2> log\n",
        "!pip install dateparser deep_translator --quiet\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVhXqza5SFCo"
      },
      "source": [
        "from __future__ import unicode_literals, print_function\n",
        "import plac\n",
        "import random\n",
        "from pathlib import Path\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "from termcolor import colored\n",
        "from deep_translator import GoogleTranslator\n",
        "import dateparser"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94LxtJpM0kMJ"
      },
      "source": [
        "##1.2 Definition of global variables "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUeKT0__a2sC"
      },
      "source": [
        "# filename of the MultiWOZ dialogue dataset\n",
        "ENGLISH_DATA_FILE = 'data.json'\n",
        "DANISH_DATA_FILE = 'danish_data.json'\n",
        "# data directory\n",
        "DATA_DIR = '/content/drive/MyDrive/ColabNotebooks/MultiWoz/'\n",
        "# dictionary where we will load the dialogue dataset\n",
        "DIALOGUE_ENG_DB = {}\n",
        "DIALOGUE_DB = {}\n",
        "# vocabulary filename\n",
        "VOCAB_FILE = 'en_50k_pruned.subword'\n",
        "# vocabulary file directory\n",
        "VOCAB_DIR = '/content/drive/MyDrive/ColabNotebooks/MultiWoz/'\n",
        "VOCAB_SIZE = 50000 #33000\n",
        "\n",
        "# load a cleaned translated train data from file DATA_DIR/TRAIN_FILENAME, \n",
        "# otherwise  generate a train data from DIALOGUE_ENG_DB\n",
        "LOAD_TRAIN_DATA = False\n",
        "# train filename\n",
        "TRAIN_FILENAME = 'label_and_train_data.json'\n",
        "# path to pretrained NER_model\n",
        "NER_MODEL_DIR = '/content/drive/MyDrive/ColabNotebooks/Spacy_NER_Model'\n",
        "\n",
        "\n",
        "\n",
        "USE_PRETRAINED_MODEL = True #use pretrained model, otherwise start from a blank model\n",
        "LOAD_MODEL = True #load pretrained model\n",
        "TRAIN = True\n",
        "N_LAYERS = 6\n",
        "TRAIN_STEPS = 100"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgGWv5ku0smh"
      },
      "source": [
        "#1.3 Loading of MultoWoz dataset, which is saved in json format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR4NoDkVbHez"
      },
      "source": [
        "# help function to load a JSON file\n",
        "def load_json(directory, file):\n",
        "    with open(f'{directory}/{file}') as file:\n",
        "        db = json.load(file)\n",
        "    return db\n",
        "\n",
        "def upload_json(directory, file, db):\n",
        "    with open(f'{directory}/{file}', mode='w') as file: \n",
        "        json.dump(db, file).encode('utf8')\n",
        "\n",
        "# load the dialogue data set into our dictionary\n",
        "DIALOGUE_ENG_DB = load_json(DATA_DIR, ENGLISH_DATA_FILE)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHIuEH8U03iG"
      },
      "source": [
        "#2.1 Development utility function for extraction conversation from the dataset and machine translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1AGmNjtbwol"
      },
      "source": [
        "def get_conversation(file, data_db):\n",
        "    '''\n",
        "    Args:\n",
        "        file (string): filename of the dialogue file saved as json\n",
        "        data_db (dict): dialogue database\n",
        "    \n",
        "    Returns:\n",
        "        string: A string containing the 'text' fields of  data[file]['log'][x]\n",
        "    '''\n",
        "    \n",
        "    # initialize empty string\n",
        "    result = ''\n",
        "    \n",
        "    # get length of file's log list\n",
        "    len_msg_log = len(data_db[file]['log'])\n",
        "    \n",
        "    # set the delimiter strings\n",
        "    delimiter_1 = ' Person 1: '\n",
        "    delimiter_2 = ' Person 2: '\n",
        "    \n",
        "    # loop over the file's log list\n",
        "    for i in range(len_msg_log):\n",
        "        \n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###\n",
        "    \n",
        "        # get i'th element of file log list\n",
        "        cur_log = data_db[file]['log'][i]['text']\n",
        "        \n",
        "        # check if i is even\n",
        "        if i%2 == 0:                   \n",
        "            # append the 1st delimiter string\n",
        "            result += delimiter_1\n",
        "        else: \n",
        "            # append the 2nd delimiter string\n",
        "            result += delimiter_2\n",
        "        \n",
        "        # append the message text from the log\n",
        "        result += cur_log\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return result\n",
        "\n",
        "#Translate:\n",
        "# Translate text\n",
        "def translate(text, target='da'):\n",
        "  if type(text) == list:\n",
        "    out = []\n",
        "    for t in text:\n",
        "      out.append(GoogleTranslator(source='auto', target=target).translate(t))\n",
        "    return out\n",
        "  return GoogleTranslator(source='auto', target=target).translate(text)\n",
        "\n",
        "def translate_db(data_db):\n",
        "  for file, val in data_db.items():\n",
        "    for i in range(len(data_db[file]['log'])):\n",
        "      src = data_db[file]['log'][i]['text']\n",
        "      # print('src=', src)\n",
        "      target = GoogleTranslator(source='auto', target='da').translate(src)\n",
        "      data_db[file]['log'][i]['text'] = target\n",
        "      # print(target)\n",
        "  return data_db\n",
        "\n",
        "# DIALOGUE_DB = translate_db(DIALOGUE_ENG_DB)\n",
        "\n",
        "# upload_json(DATA_DIR, DANISH_DATA_FILE, DIALOGUE_DB)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbKURWIz4Hb5"
      },
      "source": [
        "functions for string processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIus65GBfdoJ"
      },
      "source": [
        "import re\n",
        "# recognize_word returns the start and end indexes of all occurence of a word in a sentence.\n",
        "def recognize_word(sentence, word):\n",
        "  output = []\n",
        "  \n",
        "  for match in re.finditer(word, sentence, flags=re.IGNORECASE):\n",
        "    output.append((match.start(), match.end()))\n",
        "    # print (match.start(), match.end())\n",
        "  return output\n",
        "\n",
        "# check overlaps of [start3, end3] and [[start1, end1],[start2, end2],... ] lists\n",
        "def words_overlap(slice1, slice2):\n",
        "  \"\"\"Take two strings representing slices (e.g. 'x:y') and\n",
        "  return a boolean indicating whether they overlap\"\"\"\n",
        "  check = [None]*len(slice2)\n",
        "  for i,item in enumerate(slice2):\n",
        "    if slice1[0] < item[0]:  # slice1 is leftmost\n",
        "      check[i] = item[0] < slice1[1]  # item ends before slice1 starts\n",
        "    else:\n",
        "      check[i] = slice1[0] < item[1]\n",
        "    if check[i]:\n",
        "      return True\n",
        "  return False"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeUvYJ_k19T6",
        "outputId": "397e1aba-f410-4679-8975-4c391a40f07f"
      },
      "source": [
        "# it means [0,3] overlapes with some list(s).\n",
        "words_overlap([0,3],[[5,6],[7,8],[3,4]])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHBTA0M2gSVw",
        "outputId": "23e48fbe-353b-42c7-e5b2-cb3321de37f4"
      },
      "source": [
        "# it finds out the position of a word '5. januar' in a sentence \n",
        "recognize_word('Jeg vil reservere et hotel i Moskva fra 1. januar til 5. januar.', '5. januar')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(54, 63)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zl2Cjx2j4-_N"
      },
      "source": [
        "#2.2 Converting DIALOGUE_ENG_DB to TRAIN_DATA\n",
        "\n",
        "Here the json format of MultiWoz dataset is preprocessed for training in m "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "XT1ABwyrb7rC",
        "outputId": "8774f08d-e381-4f9a-aa38-b76e54bfe889"
      },
      "source": [
        "if LOAD_TRAIN_DATA == False:\n",
        "  i = 0\n",
        "  k = 0\n",
        "  LABELS = []\n",
        "  TRAIN_DATA = []\n",
        "  for key,val in DIALOGUE_ENG_DB.items(): #Diaologue number\n",
        "    try:\n",
        "      for ii in range(len(DIALOGUE_ENG_DB[key]['log'])): #replica number in a dialogue \n",
        "        text = translate(DIALOGUE_ENG_DB[key]['log'][ii]['text'], target='da')\n",
        "        ents_list = []\n",
        "        \n",
        "        ents_origin_list = DIALOGUE_ENG_DB[key]['log'][ii]['dialog_act']['Hotel-Inform']\n",
        "        for ient in range(len(ents_origin_list)): # entities in a replica\n",
        "          label = DIALOGUE_ENG_DB[key]['log'][ii]['dialog_act']['Hotel-Inform'][ient][0]\n",
        "          \n",
        "          val = translate(DIALOGUE_ENG_DB[key]['log'][ii]['dialog_act']['Hotel-Inform'][ient][1], target='da')\n",
        "          start_end_list = recognize_word(text, val)\n",
        "          if start_end_list != []:\n",
        "            for coinc in start_end_list:\n",
        "              if words_overlap(coinc, list(zip(*list(zip(*ents_list))[0:2]))) == False:\n",
        "                ents_list.append((coinc[0], coinc[1], label))\n",
        "          LABELS.append(label)\n",
        "        TRAIN_DATA.append( (text, {'entities':list(set(ents_list))}) )\n",
        "        k += 1\n",
        "    except:\n",
        "      pass\n",
        "    i += 1\n",
        "\n",
        "  LABELS = list(set(LABELS))\n",
        "  translated_data = {'label': LABELS, 'data':TRAIN_DATA}\n",
        "  # save translated preprocessed data\n",
        "  upload_json(DATA_DIR, TRAIN_FILENAME, translated_data)\n",
        "  print(\"The number of hotel dialogues:{}, the number of replicas: {}\\nLabels:{}\".format(i,k, LABELS))\n",
        "else:\n",
        "  translated_data = load_json(DATA_DIR, TRAIN_FILENAME)\n",
        "  LABELS = translated_data['label']\n",
        "  TRAIN_DATA = translated_data['data']"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-bbf65ebc225d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0mtranslated_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLABELS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mTRAIN_DATA\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;31m# save translated preprocessed data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m   \u001b[0mupload_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAIN_FILENAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslated_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The number of hotel dialogues:{}, the number of replicas: {}\\nLabels:{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLABELS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-1bf91bbfba6a>\u001b[0m in \u001b[0;36mupload_json\u001b[0;34m(directory, file, db)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mupload_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{directory}/{file}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# load the dialogue data set into our dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'encode'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00_xVaf_t_uV"
      },
      "source": [
        "TRAIN_DATA[0:7]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaGQcxGP4sWd"
      },
      "source": [
        "#2.2 Loadng a pretrained NER model/Creation of a new NER model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VtTH-ChSZs7"
      },
      "source": [
        "if USE_PRETRAINED_MODEL:\n",
        "  model = 'da_core_news_lg'\n",
        "else:\n",
        "  model = None\n",
        "\n",
        "output_dir=Path(NER_MODEL_DIR)\n",
        "\n",
        "#load the model\n",
        "if model is not None:\n",
        "    nlp = spacy.load(model)  \n",
        "    print(\"Loaded model '%s'\" % model)\n",
        "else:\n",
        "    nlp = spacy.blank('da')  \n",
        "    print(\"Created blank 'da' model\")\n",
        "\n",
        "#set up the pipeline\n",
        "if 'ner' not in nlp.pipe_names:\n",
        "    ner = nlp.create_pipe('ner')\n",
        "    nlp.add_pipe('ner', last=True)\n",
        "else:\n",
        "    ner = nlp.get_pipe('ner')\n",
        "\n",
        "# Add new type of labels\n",
        "for label in LABELS:\n",
        "  if label not in ner.labels:\n",
        "    ner.add_label(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMSaOrotUBr5"
      },
      "source": [
        "print(\"Pipe line names: {},\\nlabels: {}.\".format(nlp.pipe_names, ner.labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SyDcHnbaqpJ"
      },
      "source": [
        "def get_ents(nlp, text):\n",
        "  docx = nlp(text)\n",
        "  out = []\n",
        "  for token in docx.ents:\n",
        "      out.append((token.text, token.start_char, token.end_char,token.label_))\n",
        "  return out\n",
        "\n",
        "def print_ents(nlp, text):\n",
        "  docx = nlp(text)\n",
        "  for token in docx.ents:\n",
        "      print(\"text:{}, start:{}, end:{}, label:{}\".format(token.text,token.start_char, token.end_char,token.label_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65UBsm3tTDwp"
      },
      "source": [
        "Here, we want to train the recognizer by disabling the unnecessary pipeline except for NER. The nlp_update function can be used to train the recognizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrUDL7vCTEi_"
      },
      "source": [
        "from spacy.training.example import Example\n",
        "\n",
        "# for _, annotations in TRAIN_DATA:\n",
        "#     for ent in annotations.get('entities'):\n",
        "#         ner.add_label(ent[2])\n",
        "\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "    if USE_PRETRAINED_MODEL:\n",
        "      optimizer = nlp.create_optimizer()\n",
        "    else:\n",
        "      optimizer = nlp.begin_training()\n",
        "\n",
        "    for itn in range(TRAIN_STEPS):\n",
        "        random.shuffle(TRAIN_DATA)\n",
        "        losses = {}\n",
        "        for text, annotations in tqdm(TRAIN_DATA):\n",
        "          # create Example\n",
        "          doc = nlp.make_doc(text)\n",
        "          example = Example.from_dict(doc, annotations)\n",
        "          nlp.update(\n",
        "                [example],  \n",
        "                drop=0.5,  \n",
        "                sgd=optimizer,\n",
        "                losses=losses)\n",
        "        print(losses)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me_WFB3k3Qy6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ir_XiYUhTHmx"
      },
      "source": [
        "#test\n",
        "for text, _ in TRAIN_DATA[:20]:\n",
        "    doc = nlp(text)\n",
        "    print(text, 'Entities', [(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CK5Qfd4VTTgb"
      },
      "source": [
        "Finally, save the model to your path which stored in the output_dir variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gf8jE8t1TUPi"
      },
      "source": [
        "if output_dir is not None:\n",
        "    output_dir = Path(output_dir)\n",
        "    if not output_dir.exists():\n",
        "        output_dir.mkdir()\n",
        "    nlp.to_disk(output_dir)\n",
        "    print(\"Saved model to\", output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEN3s5T4VrK0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DWmBHVjcw1a"
      },
      "source": [
        "# get_ents(nlp, \"Jeg elsker Paris, men jeg kan ikke lide Frankfurt. 5. januar skal jeg besøge. december 8skal jeg besøge\")\n",
        "ents = get_ents(nlp1, \"Gonville hotel is in the expensive price range. Entities \")\n",
        "\n",
        "def extract_date(ents):\n",
        "  dates = []\n",
        "  for ent in ents:\n",
        "    parsed = dateparser.parse(ent[0])\n",
        "    if parsed:\n",
        "      dates.append(parsed)\n",
        "  return sorted(dates)\n",
        "\n",
        "def extract_date(ents):\n",
        "  dates = []\n",
        "  for ent in ents:\n",
        "    parsed = dateparser.parse(ent[0])\n",
        "    if parsed:\n",
        "      dates.append(parsed)\n",
        "  return sorted(dates)\n",
        "\n",
        "extract_date(ents), ents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SbO-lhZVz2o"
      },
      "source": [
        "ents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xu5j0rgHeE_3"
      },
      "source": [
        "# ner = nlp1.get_pipe('ner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stRaIHtherm-"
      },
      "source": [
        "ner = nlp.get_pipe('ner')\n",
        "ner.labels, nlp.pipe_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onM9erfpuqLb"
      },
      "source": [
        "# print_ents(nlp, 'Person 1: I need to book a hotel in the east that has 4 stars.')\n",
        "print_ents(nlp, 'Person 1: Washington I need to book a hotel in the east that has 4 stars. I am planing to go 11.12.21')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uldQJiigAITp"
      },
      "source": [
        "print_ents(nlp1, 'I am going to go in May 2 and May 5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5jW6To5_WKi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}